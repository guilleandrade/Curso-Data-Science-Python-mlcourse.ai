{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Assignment #2. Spring 2019\n",
    "## <center>  Competition 2. Predicting Medium articles popularity with Ridge Regression <br>(beating baselines in the \"Medium\" competition)\n",
    "    \n",
    "<img src='../../img/medium_claps.jpg' width=40% />\n",
    "\n",
    "\n",
    "In this [competition](https://www.kaggle.com/c/how-good-is-your-medium-article) we are predicting Medium article popularity based on its features like content, title, author, tags, reading time etc. \n",
    "\n",
    "Prior to working on the assignment, you'd better check out the corresponding course material:\n",
    " 1. [Classification, Decision Trees and k Nearest Neighbors](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true), the same as an interactive web-based [Kaggle Kernel](https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn) (basics of machine learning are covered here)\n",
    " 2. Linear classification and regression in 5 parts: \n",
    "    - [ordinary least squares](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols)\n",
    "    - [linear classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification)\n",
    "    - [regularization](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-3-regularization)\n",
    "    - [logistic regression: pros and cons](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit)\n",
    "    - [validation](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-5-validation)\n",
    " 3. You can also practice with demo assignments, which are simpler and already shared with solutions: \n",
    "    - \"Sarcasm detection with logistic regression\": [assignment](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit) + [solution](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution)\n",
    "    - \"Linear regression as optimization\": [assignment](https://www.kaggle.com/kashnitsky/a4-demo-linear-regression-as-optimization/edit) (solution cannot be officially shared)\n",
    "    - \"Exploring OLS, Lasso and Random Forest in a regression task\": [assignment](https://www.kaggle.com/kashnitsky/a6-demo-linear-models-and-rf-for-regression) + [solution](https://www.kaggle.com/kashnitsky/a6-demo-regression-solution)\n",
    " 4. Baseline with Ridge regression and \"bag of words\" for article content, [Kernel](https://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline)\n",
    " 5. Other [Kernels](https://www.kaggle.com/c/how-good-is-your-medium-article/kernels?sortBy=voteCount&group=everyone&pageSize=20&competitionId=8673) in this competition. You can share yours as well, but not high-performing ones (Public LB MAE shall be > 1.5). Please don't spoil the competitive spirit.  \n",
    " 6. If that's still not enough, watch two videos (Linear regression and regularization) from here [mlcourse.ai/video](https://mlcourse.ai/video), the second one on LTV prediction is smth that you won't typically find in a MOOC - real problem, real metrics, real data.\n",
    "\n",
    "**Your task:**\n",
    " 1. \"Freeride\". Come up with good features to beat the baselines \"A2 baseline (10 credits)\" (**1.45082** Public LB MAE) and \"A2 strong baseline (20 credits)\"  (**1.41117** Public LB MAE). As names suggest, you'll get 10 more credits for beating the first one, and 10 more (20 in total) for beating the second one. You need to name your [team](https://www.kaggle.com/c/how-good-is-your-medium-article/team) (out of 1 person) in full accordance with the [course rating](https://docs.google.com/spreadsheets/d/1LAy1eK8vIONzIWgcCEaVmhKPSj579zK5lrECf_tQT60/edit?usp=sharing) (for newcomers: you need to name your team with your real full name). You can think of it as a part of the assignment.\n",
    " 2. If you've beaten \"A2 baseline (10 credits)\" or performed better, you need to upload your solution as described in [course roadmap](https://mlcourse.ai/roadmap) (\"Kaggle Inclass Competition Medium\"). For all baselines that you see on Public Leaderboard, it's OK to beat them on Public LB as well. But 10 winners will be defined according to the private LB, which will be revealed by @yorko on March 11. \n",
    " \n",
    "### <center> Deadline for A2: 2019 March 10, 20:59 GMT (London time)\n",
    " \n",
    "### How to get help\n",
    "In [ODS Slack](https://opendatascience.slack.com) (if you still don't have access, fill in the [form](https://docs.google.com/forms/d/1BMqcUc-hIQXa0HB_Q2Oa8vWBtGHXk8a6xo5gPnMKYKA/edit) mentioned on the mlcourse.ai main page), we have a channel **#mlcourse_ai_news** with announcements from the course team.\n",
    "You can discuss the course content freely in the **#mlcourse_ai** channel (we still have a huge Russian-speaking group, they have a separate channel **#mlcourse_ai_rus**).\n",
    "\n",
    "Please stick this special thread for your questions:\n",
    " - [#a2_medium](https://opendatascience.slack.com/archives/C91N8TL83/p1549882568052400) \n",
    " \n",
    "Help each other without sharing actual code. Our TA Artem @datamove is there to help (only in the mentioned thread, do not write to him directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from nltk import (PorterStemmer, WordNetLemmatizer)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):\n",
    "    \n",
    "    features = ['content', 'published', 'title', 'author']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data,\n",
    "                                       '{}_{}.txt'.format(prefix, feat)),\n",
    "                          'w', encoding='utf-8')\n",
    "                     for feat in features]\n",
    "    \n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file, \\\n",
    "                open(path_to_data+'/'+prefix+'_'+'title.txt', 'w', encoding='utf-8') as out_file1, \\\n",
    "                open(path_to_data+'/'+prefix+'_'+'published.txt', 'w', encoding='utf-8') as out_file2, \\\n",
    "                open(path_to_data+'/'+prefix+'_'+'content.txt', 'w', encoding='utf-8') as out_file3, \\\n",
    "                open(path_to_data+'/'+prefix+'_'+'author.txt', 'w', encoding='utf-8') as out_file4:\n",
    "\n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            json_data = read_json_line(line)\n",
    "            \n",
    "            # You code here\n",
    "            content = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            out_file1.write(content_no_html_tags + '\\n')      \n",
    "            \n",
    "            content = json_data['published']['$date'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            out_file2.write(content_no_html_tags + '\\n')   \n",
    "            \n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            out_file3.write(content_no_html_tags + '\\n')\n",
    "                \n",
    "            content = json_data['author']['url'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            out_file4.write(content_no_html_tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_titles_from_json(path_to_inp_json_file, path_to_out_txt_file, total_length):\n",
    "    '''\n",
    "    :param path_to_inp_json_file: path to a JSON file with train/test data\n",
    "    :param path_to_out_txt_file: path to extracted features (here titles), one per line\n",
    "    :param total_length: we'll pass the hardcoded file length to make tqdm even more convenient\n",
    "    '''\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file, \\\n",
    "         open(path_to_out_txt_file, 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm_notebook(inp_file, total=total_length):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            out_file.write(content_no_html_tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dada73fac440a987692808b3bfd59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'https://medium.com/policy/medium-terms-of-service-9db0094a1e0f', '_timestamp': 1520035195.282891, '_spider': 'medium', 'url': 'https://medium.com/policy/medium-terms-of-service-9db0094a1e0f', 'domain': 'medium.com', 'published': {'$date': '2012-08-13T22:54:53.510Z'}, 'title': 'Medium Terms of Service – Medium Policy – Medium', 'content': '<div><header class=\"container u-maxWidth740\"><div class=\"uiScale uiScale-ui--regular uiScale-caption--regular postMetaHeader u-paddingBottom10 row\"><div class=\"col u-size12of12 js-postMetaLockup\"><div class=\"uiScale uiScale-ui--regular uiScale-caption--regular postMetaLockup postMetaLockup--authorWithBio u-flexCenter js-postMetaLockup\"><div class=\"u-flex0\"><a class=\"link u-baseColor--link avatar\" href=\"https://medium.com/@Medium?source=post_header_lockup\" data-action=\"show-user-card\" data-action-source=\"post_header_lockup\" data-action-value=\"504c7870fdb6\" data-action-type=\"hover\" data-user-id=\"504c7870fdb6\" dir=\"auto\"><div class=\"u-relative u-inlineBlock u-flex0\"><img src=\"https://cdn-images-1.medium.com/fit/c/120/120/1*6_fgYnisCa9V21mymySIvA.png\" class=\"avatar-image avatar-image--small\" alt=\"Go to the profile of Medium\"><div class=\"avatar-halo u-absolute u-textColorGreenNormal svgIcon\" style=\"width: calc(100% + 12px); height: calc(100% + 12px); top:-6px; left:-6px\"><svg viewbox=\"0 0 114 114\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M7.66922967,32.092726 C17.0070768,13.6353618 35.9421928,1.75 57,1.75 C78.0578072,1.75 96.9929232,13.6353618 106.33077,32.092726 L107.66923,31.4155801 C98.0784505,12.4582656 78.6289015,0.25 57,0.25 C35.3710985,0.25 15.9215495,12.4582656 6.33077033,31.4155801 L7.66922967,32.092726 Z\"></path><path d=\"M106.33077,81.661427 C96.9929232,100.118791 78.0578072,112.004153 57,112.004153 C35.9421928,112.004153 17.0070768,100.118791 7.66922967,81.661427 L6.33077033,82.338573 C15.9215495,101.295887 35.3710985,113.504153 57,113.504153 C78.6289015,113.504153 98.0784505,101.295887 107.66923,82.338573 L106.33077,81.661427 Z\"></path></svg></div></div></a></div><div class=\"u-flex1 u-paddingLeft15 u-overflowHidden\"><div class=\"u-lineHeightTightest\"><a class=\"ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker\" href=\"https://medium.com/@Medium?source=post_header_lockup\" data-action=\"show-user-card\" data-action-source=\"post_header_lockup\" data-action-value=\"504c7870fdb6\" data-action-type=\"hover\" data-user-id=\"504c7870fdb6\" dir=\"auto\">Medium</a><span class=\"followState js-followState\" data-user-id=\"504c7870fdb6\"></span></div><div class=\"ui-caption ui-xs-clamp2 postMetaInline\">Everyone’s stories and ideas</div><div class=\"ui-caption postMetaInline js-testPostMetaInlineSupplemental\"><time datetime=\"2012-08-13T22:54:53.510Z\">Aug 13, 2012</time><span class=\"middotDivider u-fontSize12\"></span><span class=\"readingTime\" title=\"5 min read\"></span></div></div></div></div></div></header><div class=\"postArticle-content js-postField js-notesSource js-trackedPost\" data-post-id=\"9db0094a1e0f\" data-source=\"post_page\" data-collection-id=\"675ebe56ac25\" data-tracking-context=\"postPage\"><section name=\"bb8c\" class=\"section section--body section--first section--last\"><div class=\"section-divider\"><hr class=\"section-divider\"></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 name=\"title\" id=\"title\" class=\"graf graf--h2 graf--leading graf--title\">Medium Terms of\\xa0Service</h1><p name=\"571b\" id=\"571b\" class=\"graf graf--p graf-after--h2\"><strong class=\"markup--strong markup--p-strong\">Effective: March 7, 2016</strong></p><p name=\"c90b\" id=\"c90b\" class=\"graf graf--p graf-after--p\">These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”).</p><p name=\"238b\" id=\"238b\" class=\"graf graf--p graf-after--p\">By using Medium, you agree to these Terms. If you don’t agree to any of the Terms, you can’t use Medium.</p><p name=\"7769\" id=\"7769\" class=\"graf graf--p graf-after--p\">We can change these Terms at any time. We keep a <a href=\"https://github.com/Medium/medium-policy\" data-href=\"https://github.com/Medium/medium-policy\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">historical</a> record of all changes to our Terms on GitHub. If a change is material, we’ll let you know before they take effect. By using Medium on or after that effective date, you agree to the new Terms. If you don’t agree to them, you should delete your account before they take effect, otherwise your use of the site and content will be subject to the new Terms.</p><h4 name=\"8c81\" id=\"8c81\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Content rights &amp; responsibilities</strong></h4><p name=\"ac74\" id=\"ac74\" class=\"graf graf--p graf-after--h4\">You own the rights to the content you create and post on Medium.</p><p name=\"651b\" id=\"651b\" class=\"graf graf--p graf-after--p\">By posting content to Medium, you give us a nonexclusive license to publish it on Medium Services, including anything reasonably related to publishing it (like storing, displaying, reformatting, and distributing it). In consideration for Medium granting you access to and use of the Services, you agree that Medium may enable advertising on the Services, including in connection with the display of your content or other information. We may also use your content to promote Medium, including its products and content. We will never sell your content to third parties without your explicit permission.</p><p name=\"2584\" id=\"2584\" class=\"graf graf--p graf-after--p\">You’re responsible for the content you post. This means you assume all risks related to it, including someone else’s reliance on its accuracy, or claims relating to intellectual property or other legal rights.</p><p name=\"c207\" id=\"c207\" class=\"graf graf--p graf-after--p\">You’re welcome to post content on Medium that you’ve published elsewhere, as long as you have the rights you need to do so. By posting content to Medium, you represent that doing so doesn’t conflict with any other agreement you’ve made.</p><p name=\"0372\" id=\"0372\" class=\"graf graf--p graf-after--p\">By posting content you didn’t create to Medium, you are representing that you have the right to do so. For example, you are posting a work that’s in the public domain, used under license (including a free license, such as <a href=\"https://creativecommons.org/licenses/\" data-href=\"https://creativecommons.org/licenses/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Creative Commons</a>), or a fair use.</p><p name=\"0472\" id=\"0472\" class=\"graf graf--p graf-after--p\">We can remove any content you post for any reason.</p><p name=\"db2b\" id=\"db2b\" class=\"graf graf--p graf-after--p\">You can delete any of your posts, or your account, anytime. Processing the deletion may take a little time, but we’ll do it as quickly as possible. We may keep backup copies of your deleted post or account on our servers for up to 14 days after you delete it.</p><h4 name=\"baf1\" id=\"baf1\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Our content and\\xa0services</strong></h4><p name=\"adc7\" id=\"adc7\" class=\"graf graf--p graf-after--h4\">We reserve all rights in Medium’s look and feel. Some parts of Medium are licensed under third-party open source licenses. We also make some of our own code available under open source licenses. As for other parts of Medium, you may not copy or adapt any portion of our code or visual design elements (including logos) without express written permission from Medium unless otherwise permitted by law.</p><p name=\"20e4\" id=\"20e4\" class=\"graf graf--p graf-after--p\">You may not do, or try to do, the following: (1) access or tamper with non-public areas of the Services, our computer systems, or the systems of our technical providers; (2) access or search the Services by any means other than the currently available, published interfaces (e.g., APIs) that we provide; (3) forge any TCP/IP packet header or any part of the header information in any email or posting, or in any way use the Services to send altered, deceptive, or false source-identifying information; or (4) interfere with, or disrupt, the access of any user, host, or network, including sending a virus, overloading, flooding, spamming, mail-bombing the Services, or by scripting the creation of content or accounts in such a manner as to interfere with or create an undue burden on the Services.</p><p name=\"f5dd\" id=\"f5dd\" class=\"graf graf--p graf-after--p\">Crawling the Services is allowed if done in accordance with the provisions of our robots.txt file, but scraping the Services is prohibited.</p><p name=\"71a8\" id=\"71a8\" class=\"graf graf--p graf-after--p\">We may change, terminate, or restrict access to any aspect of the service, at any time, without notice.</p><h4 name=\"12f1\" id=\"12f1\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">No children</strong></h4><p name=\"2ce7\" id=\"2ce7\" class=\"graf graf--p graf-after--h4\">Medium is only for people 13 years old and over. By using Medium, you affirm that you are over 13. If we learn someone under 13 is using Medium, we’ll terminate their account.</p><h4 name=\"531c\" id=\"531c\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Security</strong></h4><p name=\"3155\" id=\"3155\" class=\"graf graf--p graf-after--h4\">If you find a security vulnerability on Medium, tell us. We have a <a href=\"https://medium.com/policy/medium-s-bug-bounty-disclosure-program-34b1c80764c2\" data-href=\"https://medium.com/policy/medium-s-bug-bounty-disclosure-program-34b1c80764c2\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">bug bounty disclosure program</a>.</p><h4 name=\"05cc\" id=\"05cc\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Incorporated rules and\\xa0policies</strong></h4><p name=\"5207\" id=\"5207\" class=\"graf graf--p graf-after--h4\">By using the Services, you agree to let Medium collect and use information as detailed in our <a href=\"https://medium.com/p/f03bf92035c9\" data-href=\"https://medium.com/p/f03bf92035c9\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Privacy Policy</a>. If you’re outside the United States, you consent to letting Medium transfer, store, and process your information (including your personal information and content) in and out of the United States.</p><p name=\"6230\" id=\"6230\" class=\"graf graf--p graf-after--p\">To enable a functioning community, we have <a href=\"https://medium.com/policy/medium-rules-30e5502c4eb4\" data-href=\"https://medium.com/policy/medium-rules-30e5502c4eb4\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Rules</a>. To ensure usernames are distributed and used fairly, we have a <a href=\"https://medium.com/@Medium/medium-username-policy-7054a77fb04f\" data-href=\"https://medium.com/@Medium/medium-username-policy-7054a77fb04f\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Username Policy</a>. Under our <a href=\"https://medium.com/policy/mediums-copyright-and-dmca-policy-d126f73695\" data-href=\"https://medium.com/policy/mediums-copyright-and-dmca-policy-d126f73695\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">DMCA Policy</a>, we’ll remove material after receiving a valid takedown notice. Under our <a href=\"https://medium.com/policy/mediums-trademark-policy-e3bb53df59a7\" data-href=\"https://medium.com/policy/mediums-trademark-policy-e3bb53df59a7\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Trademark Policy</a>, we’ll investigate any use of another’s trademark and respond appropriately.</p><p name=\"21ad\" id=\"21ad\" class=\"graf graf--p graf-after--p\">By using Medium, you agree to follow these Rules and Policies. If you don’t, we may remove content, or suspend or delete your account.</p><h4 name=\"a2a2\" id=\"a2a2\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Miscellaneous</strong></h4><p name=\"b7da\" id=\"b7da\" class=\"graf graf--p graf-after--h4\"><em class=\"markup--em markup--p-em\">Disclaimer of warranty.</em> Medium provides the Services to you as is. You use them at your own risk and discretion. That means they don’t come with any warranty. None express, none implied. No implied warranty of merchantability, fitness for a particular purpose, availability, security, title or non-infringement.</p><p name=\"7073\" id=\"7073\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Limitation of Liability</em>. Medium won’t be liable to you for any damages that arise from your using the Services. This includes if the Services are hacked or unavailable. This includes all types of damages (indirect, incidental, consequential, special or exemplary). And it includes all kinds of legal claims, such as breach of contract, breach of warranty, tort, or any other loss.</p><p name=\"3d70\" id=\"3d70\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">No waiver.</em> If Medium doesn’t exercise a particular right under these Terms, that doesn’t waive it.</p><p name=\"ab04\" id=\"ab04\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Severability</em>. If any provision of these terms is found invalid by a court of competent jurisdiction, you agree that the court should try to give effect to the parties’ intentions as reflected in the provision and that other provisions of the Terms will remain in full effect.</p><p name=\"bde8\" id=\"bde8\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Choice of law and jurisdiction.</em> These Terms are governed by California law, without reference to its conflict of laws provisions. You agree that any suit arising from the Services must take place in a court located in San Francisco, California.</p><p name=\"bbb3\" id=\"bbb3\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Entire agreement.</em> These Terms (including any document incorporated by reference into them) are the whole agreement between Medium and you concerning the Services.</p><p name=\"dbf1\" id=\"dbf1\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Government use.</em> If you’re \\u200busing \\u200bMedium for the U.S. Government, <a href=\"https://medium.com/@Medium/amendment-to-medium-terms-of-service-applicable-to-u-s-government-users-fccb00db67d7\" data-href=\"https://medium.com/@Medium/amendment-to-medium-terms-of-service-applicable-to-u-s-government-users-fccb00db67d7\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">this Amendment</a> to \\u200bMedium’s Terms of Service \\u200bapplies to you\\u200b.</p><p name=\"3318\" id=\"3318\" class=\"graf graf--p graf-after--p graf--trailing\">Questions? Let us know at <a href=\"mailto:%20legal@medium.com\" data-href=\"mailto:%20legal@medium.com\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">legal@medium.com</a>.</p></div></div></section></div><footer class=\"u-paddingTop10\"><div class=\"container u-maxWidth740\"><div class=\"row\"><div class=\"col u-size12of12\"></div></div><div class=\"row\"><div class=\"col u-size12of12 js-postTags\"><div class=\"u-paddingBottom10\"><ul class=\"tags tags--postTags tags--borderless\"><li><a class=\"link u-baseColor--link\" href=\"https://medium.com/tag/terms-and-conditions?source=post\" data-action-source=\"post\">Terms And Conditions</a></li><li><a class=\"link u-baseColor--link\" href=\"https://medium.com/tag/terms?source=post\" data-action-source=\"post\">Terms</a></li><li><a class=\"link u-baseColor--link\" href=\"https://medium.com/tag/medium?source=post\" data-action-source=\"post\">Medium</a></li></ul></div></div></div><section class=\"uiScale uiScale-ui--small uiScale-caption--regular u-borderTopLightest u-marginTop10 u-paddingTop20\"><div class=\"ui-h3 u-textColorDarker u-fontSize22\">One clap, two clap, three clap, forty?</div><p class=\"ui-body u-marginBottom20 u-textColorDark u-fontSize16\">By clapping more or less, you can signal to us which stories really stand out.</p></section><div class=\"postActions js-postActionsFooter\"><div class=\"u-flexCenter\"><div class=\"u-flex1\"><div class=\"multirecommend js-actionMultirecommend u-flexCenter u-width60\" data-post-id=\"9db0094a1e0f\" data-is-icon-29px=\"true\" data-is-circle=\"true\" data-has-recommend-list=\"true\" data-source=\"post_actions_footer-----9db0094a1e0f---------------------clap_footer\"><div class=\"u-relative u-foreground\"><div class=\"clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo\" style=\"top: 14px; padding: 2px;\"></div></div><span class=\"u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft10\"></span></div></div><div class=\"buttonSet u-flex0\"></div></div></div></div><div class=\"u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer\"><div class=\"row js-postFooterInfo\"><div class=\"col u-size6of12 u-xs-size12of12\"><li class=\"uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser\"><div class=\"u-marginLeft20 u-floatRight\"><span class=\"followState js-followState\" data-user-id=\"504c7870fdb6\"></span></div><div class=\"u-tableCell\"><a class=\"link u-baseColor--link avatar\" href=\"https://medium.com/@Medium?source=footer_card\" title=\"Go to the profile of Medium\" aria-label=\"Go to the profile of Medium\" data-action-source=\"footer_card\" data-user-id=\"504c7870fdb6\" dir=\"auto\"><div class=\"u-relative u-inlineBlock u-flex0\"><img src=\"https://cdn-images-1.medium.com/fit/c/120/120/1*6_fgYnisCa9V21mymySIvA.png\" class=\"avatar-image avatar-image--small\" alt=\"Go to the profile of Medium\"><div class=\"avatar-halo u-absolute u-textColorGreenNormal svgIcon\" style=\"width: calc(100% + 12px); height: calc(100% + 12px); top:-6px; left:-6px\"><svg viewbox=\"0 0 114 114\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M7.66922967,32.092726 C17.0070768,13.6353618 35.9421928,1.75 57,1.75 C78.0578072,1.75 96.9929232,13.6353618 106.33077,32.092726 L107.66923,31.4155801 C98.0784505,12.4582656 78.6289015,0.25 57,0.25 C35.3710985,0.25 15.9215495,12.4582656 6.33077033,31.4155801 L7.66922967,32.092726 Z\"></path><path d=\"M106.33077,81.661427 C96.9929232,100.118791 78.0578072,112.004153 57,112.004153 C35.9421928,112.004153 17.0070768,100.118791 7.66922967,81.661427 L6.33077033,82.338573 C15.9215495,101.295887 35.3710985,113.504153 57,113.504153 C78.6289015,113.504153 98.0784505,101.295887 107.66923,82.338573 L106.33077,81.661427 Z\"></path></svg></div></div></a></div><div class=\"u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15\"><h3 class=\"ui-h3 u-fontSize18 u-lineHeightTighter\"><a class=\"link link--primary u-accentColor--hoverTextNormal\" href=\"https://medium.com/@Medium\" property=\"cc:attributionName\" title=\"Go to the profile of Medium\" aria-label=\"Go to the profile of Medium\" rel=\"author cc:attributionUrl\" data-user-id=\"504c7870fdb6\" dir=\"auto\">Medium</a></h3><div class=\"ui-caption u-textColorGreenNormal u-fontSize13 u-tintSpectrum u-accentColor--textNormal u-marginBottom7\">Medium member since Aug 2017</div><p class=\"ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4\">Everyone’s stories and ideas</p></div></li></div><div class=\"col u-size6of12 u-xs-size12of12 u-xs-marginTop30\"><li class=\"uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection\"><div class=\"u-marginLeft20 u-floatRight\"></div><div class=\"u-tableCell \"><a class=\"link u-baseColor--link avatar avatar--roundedRectangle\" href=\"https://medium.com/policy?source=footer_card\" title=\"Go to Medium Policy\" aria-label=\"Go to Medium Policy\" data-action-source=\"footer_card\"><img src=\"https://cdn-images-1.medium.com/fit/c/120/120/1*6_fgYnisCa9V21mymySIvA.png\" class=\"avatar-image u-size60x60\" alt=\"Medium Policy\"></a></div><div class=\"u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15\"><h3 class=\"ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4\"><a class=\"link link--primary u-accentColor--hoverTextNormal\" href=\"https://medium.com/policy?source=footer_card\" rel=\"collection\" data-action-source=\"footer_card\">Medium Policy</a></h3><p class=\"ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4\">The Fine Print</p><div class=\"buttonSet\"></div></div></li></div></div></div><div class=\"js-postFooterPlacements\"></div><div class=\"u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper\"></div><div class=\"supplementalPostContent js-heroPromo\"></div></footer></div>', 'author': {'name': None, 'url': 'https://medium.com/@Medium', 'twitter': '@Medium'}, 'image_url': None, 'tags': [], 'link_tags': {'canonical': 'https://medium.com/policy/medium-terms-of-service-9db0094a1e0f', 'publisher': 'https://plus.google.com/103654360130207659246', 'author': 'https://medium.com/@Medium', 'search': '/osd.xml', 'alternate': 'android-app://com.medium.reader/https/medium.com/p/9db0094a1e0f', 'stylesheet': 'https://cdn-static-1.medium.com/_/fp/css/main-branding-base.Ch8g7KPCoGXbtKfJaVXo_w.css', 'icon': 'https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico', 'apple-touch-icon': 'https://cdn-images-1.medium.com/fit/c/120/120/1*6_fgYnisCa9V21mymySIvA.png', 'mask-icon': 'https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg'}, 'meta_tags': {'viewport': 'width=device-width, initial-scale=1', 'title': 'Medium Terms of Service – Medium Policy – Medium', 'referrer': 'unsafe-url', 'description': 'These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…', 'theme-color': '#000000', 'og:title': 'Medium Terms of Service – Medium Policy – Medium', 'og:url': 'https://medium.com/policy/medium-terms-of-service-9db0094a1e0f', 'fb:app_id': '542599432471018', 'og:description': 'These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…', 'twitter:description': 'These Terms of Service (“Terms”) are a contract between you and A Medium Corporation. They govern your use of Medium’s sites, services, mobile apps, products, and content (“Services”). By using…', 'author': 'Medium', 'og:type': 'article', 'twitter:card': 'summary', 'article:publisher': 'https://www.facebook.com/medium', 'article:author': 'https://medium.com/@Medium', 'robots': 'index, follow', 'article:published_time': '2012-08-13T22:54:53.510Z', 'twitter:creator': '@Medium', 'twitter:site': '@Medium', 'og:site_name': 'Medium', 'twitter:label1': 'Reading time', 'twitter:data1': '5 min read', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:app:url:iphone': 'medium://p/9db0094a1e0f', 'al:ios:app_name': 'Medium', 'al:ios:app_store_id': '828256236', 'al:android:package': 'com.medium.reader', 'al:android:app_name': 'Medium', 'al:ios:url': 'medium://p/9db0094a1e0f', 'al:android:url': 'medium://p/9db0094a1e0f', 'al:web:url': 'https://medium.com/policy/medium-terms-of-service-9db0094a1e0f'}}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    with open(os.path.join('data', 'train.json'), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "        k=0\n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            while k<1:\n",
    "                json_data = read_json_line(line)\n",
    "                k+=1\n",
    "                print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'data' # modify this if you need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5349dbfe6f469e9fedda50e1dc6a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee379face364351930d9d67d6ce6290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_authors(path_to_file):\n",
    "    authors = list()\n",
    "    with open(path_to_file, encoding='utf-8') as inp_json_file:\n",
    "        for line in inp_json_file:\n",
    "            json_data = read_json_line(line)\n",
    "            authors.append(json_data['author']['url'].split('@')[1])\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_published_date(path_to_file):\n",
    "    dates = list()\n",
    "    with open(path_to_file, encoding='utf-8') as inp_json_file:\n",
    "        for line in inp_json_file:\n",
    "            json_data = read_json_line(line)\n",
    "            dates.append(json_data['published']['$date'])\n",
    "    dates_df = pd.DataFrame(dates, columns=['date'])\n",
    "    dates_df['date'] = pd.to_datetime(dates_df['date'])\n",
    "    return dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['date'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    \n",
    "    weekday = df['date'].apply(lambda ts: ts.weekday())\n",
    "    is_monday = (weekday == 0).astype('int')\n",
    "    is_tuesday = (weekday == 1).astype('int')\n",
    "    is_wednesday = (weekday == 2).astype('int')\n",
    "    is_thursday = (weekday == 3).astype('int')\n",
    "    is_friday = (weekday == 4).astype('int')\n",
    "    is_weekend = (weekday >= 5).astype('int')\n",
    "    \n",
    "    X = hstack([X_sparse,\n",
    "                morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1),\n",
    "                evening.values.reshape(-1, 1), \n",
    "                night.values.reshape(-1, 1),\n",
    "                is_monday.values.reshape(-1, 1),\n",
    "                is_tuesday.values.reshape(-1, 1),\n",
    "                is_wednesday.values.reshape(-1, 1),\n",
    "                is_thursday.values.reshape(-1, 1),\n",
    "                is_friday.values.reshape(-1, 1),\n",
    "                is_weekend.values.reshape(-1, 1)]).tocsr()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(path_to_file):\n",
    "    contents = list()\n",
    "    with open(path_to_file, encoding='utf-8') as inp_json_file:\n",
    "        for line in inp_json_file:\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content']  \n",
    "            contents.append(content)\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_features(contents):\n",
    "    content_lengths = list()\n",
    "    h1_counts = list()\n",
    "    h2_counts = list()\n",
    "    h3_counts = list()\n",
    "    img_counts = list()\n",
    "    href_counts = list()\n",
    "    \n",
    "    for content in contents:\n",
    "        content_stripped = strip_tags(content)   \n",
    "        content_length = len(content_stripped.split())\n",
    "        content_lengths.append(content_length)\n",
    "        h1_counts.append(content.count('<h1'))\n",
    "        h2_counts.append(content.count('<h2'))\n",
    "        h3_counts.append(content.count('<h3'))\n",
    "        img_counts.append(content.count('<img'))\n",
    "        href_counts.append(content.count('<href'))\n",
    "        \n",
    "    counts = np.hstack([np.array(h1_counts).reshape(-1, 1),\n",
    "                    np.array(h2_counts).reshape(-1, 1),\n",
    "                    np.array(h3_counts).reshape(-1, 1),\n",
    "                    np.array(img_counts).reshape(-1, 1),\n",
    "                    np.array(href_counts).reshape(-1, 1)])\n",
    "    \n",
    "    content_lengths = np.array(content_lengths)\n",
    "    is_short = (content_lengths<1350).astype('int')\n",
    "    is_medium = ((content_lengths>=1350) & (content_lengths<2700)).astype('int')\n",
    "    is_long = ((content_lengths>=2700) & (content_lengths<6750)).astype('int')\n",
    "    is_huge = (content_lengths>=6750).astype('int')\n",
    "    \n",
    "    length_types = np.hstack([is_short.reshape(-1, 1),\n",
    "                              is_medium.reshape(-1, 1),\n",
    "                              is_long.reshape(-1, 1),\n",
    "                              is_huge.reshape(-1, 1) ])\n",
    "    \n",
    "    return counts, length__types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(path_to_file):\n",
    "    titles = list()\n",
    "    with open(path_to_file, encoding='utf-8') as inp_json_file:\n",
    "        for line in inp_json_file:\n",
    "            json_data = read_json_line(line)\n",
    "            title = json_data['title']\n",
    "            titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_features(titles):\n",
    "    titles_lengths = np.array([len(title.split()) for title in titles])\n",
    "    is_short = (titles_lengths<6).astype('int')\n",
    "    is_medium = ((titles_lengths>=6) & (titles_lengths<11)).astype('int')\n",
    "    is_long = ((titles_lengths>=11) & (titles_lengths<20)).astype('int')\n",
    "    is_huge = (titles_lengths>=20).astype('int')\n",
    "    \n",
    "    length_types = np.hstack([is_short.reshape(-1, 1),\n",
    "                              is_medium.reshape(-1, 1),\n",
    "                              is_long.reshape(-1, 1),\n",
    "                              is_huge.reshape(-1, 1) ])\n",
    "    return length_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmingLemmatizingTokenizer(object):\n",
    "    \n",
    "    def __init__(self, stemmer=PorterStemmer, lemmatizer=WordNetLemmatizer):\n",
    "        self.stemmer = stemmer()\n",
    "        self.lemmatizer = lemmatizer()\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        # strings of punctuation signs and digits\n",
    "        from string import punctuation, digits\n",
    "        # some other unicode chars i found in the content\n",
    "        other_unicode_chars = '’’”“\\u200b'\n",
    "        chars_to_remove = ''.join((punctuation,\n",
    "                                   digits,\n",
    "                                   other_unicode_chars))\n",
    "        # getting rid of punctuation signs and digits\n",
    "        transtab = str.maketrans(chars_to_remove, ' '*len(chars_to_remove))\n",
    "        # goiinf through all tokens with 3 or more chars\n",
    "        # lemmatizing the verbs first, then stemming all words\n",
    "        return [self.stemmer.stem(self.lemmatizer.lemmatize(token, pos='v')) \n",
    "                for token in word_tokenize(doc.translate(transtab)) \n",
    "                if len(token) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "temp = []\n",
    "s = StemmingLemmatizingTokenizer()\n",
    "for eggs in stop_words:\n",
    "    token = s(eggs)\n",
    "    if token:\n",
    "        temp += token\n",
    "stop_words = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_vectorizer = CountVectorizer()\n",
    "counts_scaler = StandardScaler()\n",
    "content_vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                                     tokenizer=StemmingLemmatizingTokenizer(),\n",
    "                                     stop_words=stop_words,\n",
    "                                     max_features=100000)\n",
    "title_vectorizer = TfidfVectorizer(ngram_range=(2, 3), \n",
    "                                   tokenizer=StemmingLemmatizingTokenizer(),\n",
    "                                   stop_words=stop_words,\n",
    "                                   max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_DATA = 'data' # modify this if you need to\n",
    "TARGET_FILE = 'train_log1p_recommends.csv'\n",
    "TARGET_PATH = os.path.join(PATH_TO_DATA, TARGET_FILE)\n",
    "TRAIN_FILE = 'train.json'\n",
    "TRAIN_PATH = os.path.join(PATH_TO_DATA, TRAIN_FILE)\n",
    "TEST_FILE = 'test.json'\n",
    "TEST_PATH = os.path.join(PATH_TO_DATA, TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "authors = get_authors(TRAIN_PATH)\n",
    "author_sparse = author_vectorizer.fit_transform(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "date_df = extract_published_date(TRAIN_PATH)\n",
    "train_data = add_time_features(date_df, author_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "raw_contents = get_contents(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "#counts, length_types = get_content_features(raw_contents)\n",
    "#counts_scaled = counts_scaler.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 50min 35s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "content_sparse = content_vectorizer.fit_transform((strip_tags(content) \n",
    "                                                   for content in raw_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "titles = get_titles(TRAIN_PATH)\n",
    "title_length_types = get_title_features(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "title_sparse = title_vectorizer.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_sparse = hstack([train_data,\n",
    "                     title_sparse,\n",
    "                     title_length_types,\n",
    "                     content_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "authors = get_authors(TEST_PATH)\n",
    "author_sparse = author_vectorizer.transform(authors)\n",
    "\n",
    "date_df = extract_published_date(TEST_PATH)\n",
    "test_data = add_time_features(date_df, author_sparse)\n",
    "\n",
    "raw_contents = get_contents(TEST_PATH)\n",
    "#counts, length_types = get_content_features(raw_contents)\n",
    "#counts_scaled = counts_scaler.transform(counts)\n",
    "\n",
    "stripped_contents = [strip_tags(content) for content in raw_contents]\n",
    "content_sparse = content_vectorizer.transform(stripped_contents)\n",
    "\n",
    "titles = get_titles(TEST_PATH)\n",
    "title_length_types = get_title_features(titles)\n",
    "title_sparse = title_vectorizer.transform(titles)\n",
    "\n",
    "X_test_sparse = hstack([test_data, \n",
    "                    title_sparse,\n",
    "                    title_length_types,\n",
    "                    content_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here\n",
    "\n",
    "\n",
    "#X_train_time_features_sparse\n",
    "\n",
    "\n",
    "\n",
    "#X_test_time_features_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_sparse = hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "#                         X_train_author_sparse, \n",
    "#                         X_train_time_features_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_sparse = hstack([X_test_content_sparse, X_test_title_sparse,\n",
    "#                        X_test_author_sparse, \n",
    "#                        X_test_time_features_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here %%time\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train_part_sparse, y_train_part);\n",
    "ridge_pred = ridge.predict(X_valid_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_valid, bins=30, alpha=.5, color='red',\n",
    "         label='true values', range=(0,10));\n",
    "plt.hist(ridge_pred, bins=30, alpha=.5, color='green',\n",
    "         label='predicted values', range=(0,10));\n",
    "plt.legend();\n",
    "#valid_mae = mean_absolute_error(y_valid, ridge_pred)\n",
    "#print(valid_mae, np.expm1(valid_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train_sparse, y_train);\n",
    "ridge_test_pred = ridge.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred, os.path.join(PATH_TO_DATA,\n",
    "                                                    'assignment2_medium_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now's the time for dirty Kaggle hacks. Form a submission file with all zeros. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?**\n",
    "\n",
    "**UPD:** There is a [tutorial](https://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_english/tutorials/kaggle_leaderboard_probing_nikolai_timonin.ipynb) on leaderboard probing which is written within mlcourse.ai and is relevant here. (Originally, contestants were supposed to come up with simple probing techniques on their own. But now when this tutorial is shared, we eliminate \"discovery bias\" and equalize everybody's chances by sharing this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(np.zeros_like(ridge_test_pred), \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'medium_all_zeros_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.183488\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones = pd.DataFrame(ridge_test_pred, index=)\n",
    "predicciones.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here\n",
    "ridge_test_pred_modif = ridge_test_pred + 4.3333 - 3.1835\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred_modif, \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'assignment2_medium_submission_with_hack.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "<img src='../../img/kaggle_shakeup.png' width=50%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
